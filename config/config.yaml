model:
  img_size: 224            # Image size
  patch_size: 16           # Patch size
  num_frames: 16           # Number of frames
  tubelet_size: 4          # Tubelet size
  embed_dim: 768           # Embedding dimension for both encoder and embeddings
  frame_interval: 1        # Frame interval for the tubelets

  num_layers_encoder: 12   # Number of layers in the encoder
  encoder_num_heads: 12    # Number of attention heads in the encoder
  encoder_mlp_ratio: 4     # MLP ratio in the encoder

  decoder_embed_dim: 768   # Embedding dimension for the decoders
  num_layers_decoder: 4    # Number of layers in both decoders
  decoder_num_heads: 8     # Number of attention heads in the decoders
  decoder_mlp_ratio: 4     # MLP ratio in the decoders

training:
  batch_size: 4
  num_epochs: 500
  lr: 0.00003
  weight_decay: 0.0001
  model_checkpoint_path: ../checkpoints
  num_workers: 0
  mask_ratio: 0.95         # Mask ratio for training
  alpha: 1.0               # Weight for RGB loss
  beta: 1.0                # Weight for Depth loss 

data:
  depth_model_checkpoint: ../depth_anything_v2/checkpoints/depth_anything_v2_vitl.pth
  finevideo_path: /data/datasets/finevideo/sports_videos
  depth_stats:
    mean: 0.5
    std: 0.5

wandb: 
  entity: Neilus03
  #entity: cvc-mireia
  project: VD-MAE
  name: pretraining
  log_interval: 1